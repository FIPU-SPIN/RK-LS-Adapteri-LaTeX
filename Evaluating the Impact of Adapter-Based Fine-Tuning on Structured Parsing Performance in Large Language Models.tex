\documentclass[fleqn,10pt]{olplainarticle}
% Use option lineno for line numbers 

\title{ Evaluating the Impact of Adapter-Based Fine-Tuning on Structured Parsing Performance in Large Language Models}

\author[1]{Ratomir Karlović}
\author[2]{Luka Sever}
\affil[1]{Address of first author}
\affil[2]{Address of second author}

\keywords{LLM, Adapters, PEFT}

\begin{document}

\begin{abstract}
Recent advances in large language models (LLMs) highlight two dominant strategies for performance improvement: prompt engineering and fine-tuning. While prompt design can significantly influence model output, it remains uncertain whether lightweight fine-tuning methods,such as adapter-based training, offer meaningful advantages for structured, domain-specific tasks.
This study builds on prior research comparing three prompting strategies for natural-language command parsing into JSON schemas. Expanding that framework, the current work investigates how adapter-based fine-tuning, where most model parameters are frozen and only small adapter modules are trained, affects model accuracy, consistency, and robustness.
The experiment uses the same controlled shopping-cart parsing task and dataset of 1000 synthetic commands to ensure direct comparability. Results will quantify the trade-off between computational cost and performance gains, offering evidence-based insights into whether fine-tuning is a justified investment compared to advanced prompt engineering.
The expected contribution is a clear, empirical framework for deciding when fine-tuning meaningfully enhances LLM utility in applied natural-language understanding.

\end{abstract}

\flushbottom
\maketitle
\thispagestyle{empty}

\section{Introduction}

The rapid evolution of Large Language Models (LLMs) has fundamentally transformed the landscape of Natural Language Processing (NLP), enabling advanced capabilities in reasoning, content generation, and semantic understanding \citep{ling2025domain}. While early applications focused primarily on open-ended text generation, modern production environments increasingly require LLMs to function as reliable semantic parsers capable of mapping unstructured user intent into rigorous, machine-readable formats such as JSON or SQL \citep{han2024parameter}. This capability is particularly critical in domain-specific applications, such as e-commerce controllers, where valid schema adherence is a prerequisite for system functionality.
A primary challenge in deploying these systems is the "optimization dilemma": choosing between inference-time strategies and model training. As highlighted in our previous survey \citep{karlovic2025large}, while prompt engineering offers a lightweight "black-box" approach to adaptation, it often struggles with robustness when constrained to rigid schema definitions. Conversely, full-parameter fine-tuning yields high precision but incurs prohibitive computational costs and memory requirements \citep{mundra2024comprehensive}. To address this, Parameter-Efficient Fine-Tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), have emerged as a middle ground, freezing pre-trained weights and injecting trainable rank-decomposition matrices to approximate full fine-tuning performance with a fraction of the resources \citep{hu2023llm}.
To address these challenges, this study evaluates the cost-benefit ratio of fine-tuning against prompting for high-fidelity semantic parsing. Building upon the theoretical frameworks established in our prior work \citep{karlovic2025large}, we implement a controlled experimental design using the LLaMA-3-8B model. The system compares two distinct optimization paradigms: Advanced Prompt Engineering and Adapter-Based Fine-Tuning, utilizing LoRA and its quantized variant (QLoRA) to update the model's internal representations \citep{patil2025analyzing}. This setup allows for a direct comparison of how each method handles the rigorous constraints of mapping natural language commands to structured JSON outputs.
The main aim of this research is to identify the "break-even point" where the computational investment of training adapters yields a statistically significant advantage over advanced prompting. The main contribution of this study is a structured empirical framework for benchmarking LLaMA-3 on structured parsing tasks, providing evidence-based insights into whether the resource consumption of fine-tuning is justified by gains in accuracy and robustness. The evaluation reveals that while prompting is sufficient for simple intents, adapter-based methods significantly outperform in handling complex, multi-intent schemas, particularly when targeting all linear layers with optimized hyperparameters.


\section{Background and Related Works}

The rapid growth of large language models (LLMs) has necessitated strategies for adapting them efficiently to downstream tasks. Zhiqiang Hu et al. \citep{hu2023llm} introduced LLM-Adapters, a framework for parameter-efficient fine-tuning (PEFT) that integrates Series adapters, Parallel adapters, Prefix-Tuning, and LoRA. Their study demonstrated that smaller LLaMA models (7B and 13B) equipped with optimized adapters could match or surpass the performance of much larger models such as ChatGPT and GPT-3.5, particularly on structured reasoning tasks using datasets like Math10K and Commonsense170K. These findings suggest that adapter-based PEFT can improve structured task accuracy compared to advanced prompting strategies.

Ding et al. \citep{ding2023parameter} proposed a unified framework called delta-tuning, which adjusts only a small fraction of model parameters. Through extensive evaluation on over 100 NLP tasks, they found that delta-tuning achieved performance comparable to full fine-tuning while significantly reducing GPU memory usage and accelerating backpropagation. Larger models not only benefited more from minimal parameter updates but also converged faster, reinforcing the potential of adapter-based methods to enhance task-specific performance efficiently.

Esmaeili et al. \citep{esmaeili2026empirical} conducted empirical studies on PEFT for code-specific LLMs, examining LoRA, Compacter, and IA3. Their findings showed that LoRA consistently outperformed other adapters in code summarization and generation, occasionally exceeding full fine-tuning on low-resource languages like R. The study highlighted that the number of trainable parameters influenced functional correctness more than the adapter architecture itself, further supporting the effectiveness of PEFT in improving structured outputs in specialized domains.

Razuvayevskaya et al. \citep{razuvayevskaya2024comparison} compared parameter-efficient techniques and full fine-tuning for multilingual news classification. Using XLM-RoBERTa Large, they showed that LoRA and bottleneck adapters could reduce trainable parameters by 140–280 times while maintaining competitive accuracy. Their results suggested that PEFT strategies can outperform traditional prompting approaches, especially when sufficient source data is available.

Shin et al. \citep{shin2025prompt} assessed the trade-offs between prompt engineering and fine-tuning for automated code tasks. Their evaluation of GPT-4 against seventeen fine-tuned baselines revealed that while task-specific prompting sometimes outperformed automated methods in code summarization, fine-tuned models were generally superior in code generation. Human-in-the-loop conversational prompting further enhanced outcomes, demonstrating that adapter-based fine-tuning can stabilize and improve output quality over prompting alone.

Ming Gong et al. \citep{gong2025structure} developed structure-learnable adapters, which introduced differentiable gating and sparsity control to dynamically optimize adapter placement and activation paths. Experiments on the Multi-Task NLU Benchmark indicated that this approach not only achieved high accuracy with minimal parameters but also reduced output variability and maintained robustness under noisy conditions, supporting the notion that adapter fine-tuning can yield more consistent outputs than prompting.

Fang and Ye \citep{fangtowards} proposed RFedLR, a robust PEFT framework for federated learning. By selectively updating noise-sensitive parameters and dynamically weighting client updates, RFedLR achieved up to 10.15\% accuracy improvements over state-of-the-art baselines under high-noise conditions while using only 0.1754\% of the trainable parameters. These results reinforce that adapter-based fine-tuning provides more stable and reliable outputs than prompt-based methods, even in decentralized and noisy environments.

Shuo Chen et al. \citep{chen2023benchmarking} benchmarked eleven adaptation methods on vision-language models under textual and visual corruptions. They found that parameter-efficient adapters exhibited higher resilience to input perturbations than full fine-tuning or prompting, although no single method dominated across all datasets. Increasing adaptation data or parameter size did not guarantee robustness, highlighting the importance of careful PEFT design to enhance model stability.

Chen et al. \citep{chen2025evaluation} evaluated prompt engineering for industrial document processing tasks and found that few-shot learning improved reasoning capabilities over zero-shot prompts. However, adapter-based PEFT consistently offered more reliable and less variable outputs, particularly in complex scenarios with noisy OCR inputs.

Kaijie Zhu et al. \citep{zhu2023promptrobust} introduced PromptRobust, a benchmark for adversarial prompt evaluation across multiple LLMs. Their results showed that word-level adversarial prompts caused a 39\% average performance drop, and few-shot prompts exhibited better robustness than zero-shot prompting. The study illustrates that adapter fine-tuning can improve resilience to prompt-based perturbations.

Jaehyung Kim et al. \citep{kim2023roast} presented ROAST, combining adversarial perturbations with selective training to improve robustness across sentiment classification and entailment tasks. ROAST yielded average improvements of 18.39\% and 7.63\% over state-of-the-art baselines, demonstrating that adapter fine-tuning can mitigate input noise effects more effectively than prompting.

Pei et al. \citep{pei2025selfprompt} proposed SelfPrompt, which autonomously evaluates LLM robustness using domain-constrained knowledge graphs and adversarial prompt generation. Their findings indicated that domain-specific adapter fine-tuning enhances robustness in specialized fields like medicine and biology, outperforming prompting strategies alone.

Lin Mu et al. \citep{mu2025robustness} introduced Robustness of Prompting (RoP), a parameter-free method for improving LLM resilience through error correction and guidance prompts. While effective, experiments confirmed that adapter fine-tuning provides superior stability and transferability across architectures, emphasizing the practical value of PEFT in noisy real-world environments.

Sajjadi Mohammadabadi et al. \citep{sajjadi2025survey} surveyed LLM architectures and adaptation methods, emphasizing the efficiency of PEFT, instruction tuning, and RLHF. They noted that adapter-based fine-tuning can deliver significant performance gains relative to prompting, justifying its computational cost in applied settings.

Trad and Chehab \citep{trad2024prompt} studied phishing detection LLMs and found that while refined prompting improved performance, fine-tuned models achieved higher F1-scores and superior reliability, demonstrating that the cost of PEFT is justified when high precision is required.

Pornprasit and Tantithamthavorn \citep{pornprasit2024fine} evaluated LLM-based code review, showing that fine-tuning GPT-3.5 achieved 73–74\% higher Exact Match rates than prompting approaches. Their study confirmed that adapter-based fine-tuning can deliver meaningful gains even with modest training data.

Wang et al. \citep{wang2025cosmos} introduced COSMOS, a framework for predicting adaptation outcomes with minimal trials. Their results indicated that fine-tuning consistently outperformed in-context learning in medium to high-cost scenarios, supporting the view that adapter PEFT justifies its resource investment by providing predictable and superior performance.

Based on the cumulative evidence from these studies, the overarching goal of this work is to investigate how adapter-based parameter-efficient fine-tuning (PEFT) compares to advanced prompting in terms of structured task accuracy, output stability, robustness, and cost-effectiveness. Accordingly, the following research hypotheses are formulated:

\begin{itemize}
    \item \textbf{H1:} Adapter-based PEFT improves structured task performance relative to advanced prompting.
    \item \textbf{H2:} Adapter fine-tuning reduces output variability compared to prompting approaches.
    \item \textbf{H3:} Adapter fine-tuning enhances robustness to input noise, adversarial perturbations, and paraphrasing relative to prompting.
    \item \textbf{H4:} Performance gains achieved via adapter-based PEFT justify the computational and resource cost compared to advanced prompting methods.
\end{itemize}

\section{Methodology}

This section details the experimental framework designed to evaluate the trade-offs between parameter-efficient fine-tuning (PEFT) and advanced in-context learning (ICL). The primary objective is to determine the performance ceiling, robustness, and structural integrity of various Large Language Models (LLMs) when tasked with converting unstructured natural language into strictly formatted JSON objects. This methodology extends the comparative benchmarks established in \citep{karlovic2025large}, applying them to a specialized domain of structured parsing.

\subsection{Dataset and Task Definition}
The experimental task focuses on the "Natural Language to Structured Command" conversion, a critical component in automated inventory and commerce systems. The dataset consists of 10,000 synthetically generated yet linguistically diverse shopping-cart instructions.
Each instance requires the model to extract or infer three specific attributes:

\begin{enumerate}
    \item \textbf{Action:} A classification task mapping verbs to a binary set of \{\textit{add, remove}\}.
    \item \textbf{Product:} A named entity recognition (NER) task to identify the subject item.
    \item \textbf{Quantity:} A numerical extraction task where the model must default to an integer value of 1 if no quantifier is explicitly mentioned in the prompt.
\end{enumerate}

The corpus was split into a \textbf{Training Set} of 8,000 examples, utilized for the LoRA adapter training phase, and a \textbf{Test Set} of 2,000 examples. The test set remained unseen by the adapters to ensure unbiased evaluation against the prompting strategies.

\subsection{Model Selection and Inference Environment}
We selected a heterogeneous suite of models to analyze how architectural scale and specialized training (e.g., reasoning-distilled models) influence parsing performance. The models were deployed via the Ollama framework to ensure local reproducibility and consistent inference parameters (e.g., temperature set to $0$ to minimize stochastic variance).

The model suite is categorized by parameter count and family:
\begin{itemize}
    \item \textbf{Large-Scale:} Llama 3.3 (70B).
    \item \textbf{Mid-Range:} DeepSeek-R1 (14B), Phi-4 (14B), Llama 3.1 (8B), Qwen 3 (8B), and Granite 3.3 (8B).
    \item \textbf{Small-Scale (SLMs):} Mistral (7B), Qwen 3 (4B), Gemma 3 (4B), Llama 3.2 (3B), and DeepSeek-R1 (1.5B).
\end{itemize}

\subsection{Adapter-Based Fine-Tuning (LoRA)}
For the fine-tuning paradigm, we employed \textbf{Low-Rank Adaptation (LoRA)}. This technique was chosen for its ability to maintain the underlying general knowledge of the base model while specializing its output structure for JSON parsing with minimal computational overhead.

The LoRA approach modifies the pre-trained weight matrices $W_0 \in \mathbb{R}^{d \times k}$ by adding a low-rank decomposition $BA$, where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$. The forward pass for a given input $x$ is represented as:
\begin{equation}
    h = W_0 x + \Delta W x = W_0 x + BAx
\end{equation}

In this study, we targeted all linear modules within the transformer architecture (specifically \texttt{q\_proj}, \texttt{v\_proj}, \texttt{k\_proj}, \texttt{o\_proj}, and \texttt{MLP} layers) to ensure the adapter captured the necessary structural constraints for JSON generation. The specific hyperparameters utilized in this study include:
\begin{itemize}
    \item \textbf{Rank ($r$):} [Insert Value]
    \item \textbf{Alpha ($\alpha$):} [Insert Value]
    \item \textbf{Target Modules:} All linear layers (comprehensive adaptation).
    \item \textbf{Precision:} 4-bit (QLoRA) or 16-bit, depending on the hardware constraints at runtime.
\end{itemize}
\subsection{Prompt Engineering Strategies}
To establish a baseline for In-Context Learning (ICL), we implemented three escalating levels of prompt complexity:

\begin{enumerate}
    \item \textbf{Minimal Instruction (Zero-Shot):} A concise prompt defining the role of the model and the required JSON schema.
    \item \textbf{Extended Prompt (Instructional Weight):} This strategy includes detailed edge-case instructions, emphasizing the default quantity logic and strict "no-prose" constraints to prevent conversational "chatter."
    \item \textbf{Few-Shot Prompting:} Building upon the Extended Prompt, this version includes [Insert Number] high-quality examples of natural language inputs and their corresponding JSON outputs, providing a semantic and structural blueprint for the model.
\end{enumerate}

\subsection{Evaluation Metrics}
The models are evaluated on two primary dimensions: \textbf{Syntactic Integrity} and \textbf{Semantic Accuracy}.

\subsubsection{JSON Validity Rate}
Before measuring accuracy, we assess if the output is a "well-formed" JSON object. Any output that fails standard library parsing ($json.loads()$) is categorized as a failure.

\subsubsection{Structured F1 Score}
For all valid JSON outputs, we calculate the F1 score. This requires an exact match between the predicted value and the ground truth for each of the three fields. The field-level F1 score is defined as the harmonic mean of precision ($P$) and recall ($R$):
\begin{equation}
    F_1 = 2 \cdot \frac{P \cdot R}{P + R}
\end{equation}

We report the macro-averaged F1 across all fields to provide a singular measure of parsing performance.
\section{Results and discussion}
\section{Conclusion}


\section{Acknowledgments}

This research is (partly) supported by SPIN projects “INFOBIP Konverzacijski Order Management (IP.1.1.03.0120)”, “Projektiranje i razvoj nove generacije laboratorijskog informacijskog sustava (iLIS)” (IP.1.1.03.0158), “Istraživanje i razvoj inovativnog sustava preporuka za napredno gostoprimstvo u turizmu (InnovateStay)” (IP.1.1.03.0039), “EDIH ADRIA – European Digital Innovation Hub Adria 2.0 (project no. 101256325)” and the FIPU project “Sustav za modeliranje i provedbu poslovnih procesa u heterogenom i decentraliziranom računalnom sustavu”.

% \bibliographystyle{plainnat}
\bibliography{references}
\end{document}