@article{ling2025domain,
  title={Domain specialization as the key to make large language models disruptive: A comprehensive survey},
  author={Ling, Chen and Zhao, Xujiang and Lu, Jiaying and Deng, Chengyuan and Zheng, Can and Wang, Junxiang and Chowdhury, Tanmoy and Li, Yun and Cui, Hejie and Zhang, Xuchao and others},
  journal={ACM Computing Surveys},
  volume={58},
  number={3},
  pages={1--39},
  year={2025},
  publisher={ACM New York, NY}
}

@article{han2024parameter,
  title={Parameter-efficient fine-tuning for large models: A comprehensive survey},
  author={Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},
  journal={arXiv preprint arXiv:2403.14608},
  year={2024}
}

@article{karlovic2025large,
  title={Large language models as Retail Cart Assistants: A Prompt-Based Evaluation},
  author={Karlovi{\'c}, Ratomir and Lorencin, Ivan},
  journal={Human Systems Engineering and Design (IHSED2025): Future Trends and Applications},
  volume={198},
  year={2025},
  publisher={Orlando (FL): Applied Human Factors and Ergonomics International-AHFE Open~â€¦}
}

@inproceedings{mundra2024comprehensive,
  title={A comprehensive analysis of adapter efficiency},
  author={Mundra, Nandini and Doddapaneni, Sumanth and Dabre, Raj and Kunchukuttan, Anoop and Puduppully, Ratish and Khapra, Mitesh M},
  booktitle={Proceedings of the 7th Joint International Conference on Data Science \& Management of Data (11th ACM IKDD CODS and 29th COMAD)},
  pages={136--154},
  year={2024}
}

@inproceedings{hu2023llm,
  title={Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models},
  author={Hu, Zhiqiang and Wang, Lei and Lan, Yihuai and Xu, Wanyu and Lim, Ee-Peng and Bing, Lidong and Xu, Xing and Poria, Soujanya and Lee, Roy},
  booktitle={Proceedings of the 2023 conference on empirical methods in natural language processing},
  pages={5254--5276},
  year={2023}
}

@article{patil2025analyzing,
  title={Analyzing LLAMA3 Performance on Classification Task Using LoRA and QLoRA Techniques},
  author={Patil, Rajvardhan and Khot, Priyanka and Gudivada, Venkat},
  journal={Applied Sciences},
  volume={15},
  number={6},
  pages={3087},
  year={2025},
  publisher={MDPI}
}

@article{ding2023parameter,
  title={Parameter-efficient fine-tuning of large-scale pre-trained language models},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={Nature machine intelligence},
  volume={5},
  number={3},
  pages={220--235},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{esmaeili2026empirical,
  title={Empirical studies of parameter efficient methods for large language models of code and knowledge transfer to r},
  author={Esmaeili, Amirreza and Saberi, Iman and Fard, Fatemeh},
  journal={Empirical Software Engineering},
  volume={31},
  number={2},
  pages={30},
  year={2026},
  publisher={Springer}
}

@article{razuvayevskaya2024comparison,
  title={Comparison between parameter-efficient techniques and full fine-tuning: A case study on multilingual news article classification},
  author={Razuvayevskaya, Olesya and Wu, Ben and Leite, Jo{\~a}o A and Heppell, Freddy and Srba, Ivan and Scarton, Carolina and Bontcheva, Kalina and Song, Xingyi},
  journal={Plos one},
  volume={19},
  number={5},
  pages={e0301738},
  year={2024},
  publisher={Public Library of Science San Francisco, CA USA}
}

@inproceedings{shin2025prompt,
  title={Prompt Engineering or Fine-Tuning: An Empirical Assessment of LLMs for Code},
  author={Shin, Jiho and Tang, Clark and Mohati, Tahmineh and Nayebi, Maleknaz and Wang, Song and Hemmati, Hadi},
  booktitle={2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR)},
  pages={490--502},
  year={2025},
  organization={IEEE}
}

@inproceedings{gong2025structure,
  title={Structure-learnable adapter fine-tuning for parameter-efficient large language models},
  author={Gong, Ming and Deng, Yingnan and Qi, Nia and Zou, Yujun and Xue, Zhihao and Zi, Yun},
  booktitle={International Conference on Artificial Intelligence and Computational Engineering (AICE 2025)},
  volume={2025},
  pages={225--230},
  year={2025},
  organization={IET}
}

@inproceedings{fangtowards,
  title={Towards Robust Parameter-Efficient Fine-Tuning for Federated Learning},
  author={Fang, Xiuwen and Ye, Mang},
  year={2025},
  booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems}
}

@article{chen2023benchmarking,
  title={Benchmarking robustness of adaptation methods on pre-trained vision-language models},
  author={Chen, Shuo and Gu, Jindong and Han, Zhen and Ma, Yunpu and Torr, Philip and Tresp, Volker},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={51758--51777},
  year={2023}
}

@article{chen2025evaluation,
  title={Evaluation of Prompt Engineering on the Performance of a Large Language Model in Document Information Extraction},
  author={Chen, Lun-Chi and Weng, Hsin-Tzu and Pardeshi, Mayuresh Sunil and Chen, Chien-Ming and Sheu, Ruey-Kai and Pai, Kai-Chih},
  journal={Electronics},
  volume={14},
  number={11},
  pages={2145},
  year={2025},
  publisher={MDPI}
}

@inproceedings{zhu2023promptrobust,
  title={Promptrobust: Towards evaluating the robustness of large language models on adversarial prompts},
  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Zhang, Yue and Gong, Neil and others},
  booktitle={Proceedings of the 1st ACM workshop on large AI systems and models with privacy and safety analysis},
  pages={57--68},
  year={2023}
}

@inproceedings{kim2023roast,
  title={RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training},
  author={Kim, Jaehyung and Mao, Yuning and Hou, Rui and Yu, Hanchao and Liang, Davis and Fung, Pascale and Wang, Qifan and Feng, Fuli and Huang, Lifu and Khabsa, Madian},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={3412--3444},
  year={2023}
}

@inproceedings{pei2025selfprompt,
  title={Selfprompt: Autonomously evaluating llm robustness via domain-constrained knowledge guidelines and refined adversarial prompts},
  author={Pei, Aihua and Yang, Zehua and Zhu, Shunan and Cheng, Ruoxi and Jia, Ju},
  booktitle={Proceedings of the 31st International Conference on Computational Linguistics},
  pages={6840--6854},
  year={2025}
}

@article{mu2025robustness,
  title={Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks},
  author={Mu, Lin and Chu, Guowei and Ni, Li and Sang, Lei and Wu, Zhize and Jin, Peiquan and Zhang, Yiwen},
  journal={arXiv preprint arXiv:2506.03627},
  year={2025}
}

@article{sajjadi2025survey,
  title={A survey of large language models: evolution, architectures, adaptation, benchmarking, applications, challenges, and societal implications},
  author={Sajjadi Mohammadabadi, Seyed Mahmoud and Kara, Burak Cem and Eyupoglu, Can and Uzay, Can and Tosun, Mehmet Serkan and Karaku{\c{s}}, Oktay},
  journal={Electronics},
  volume={14},
  number={18},
  year={2025},
  publisher={MDPI}
}

@article{trad2024prompt,
  title={Prompt engineering or fine-tuning? a case study on phishing detection with large language models},
  author={Trad, Fouad and Chehab, Ali},
  journal={Machine Learning and Knowledge Extraction},
  volume={6},
  number={1},
  pages={367--384},
  year={2024},
  publisher={MDPI}
}

@article{pornprasit2024fine,
  title={Fine-tuning and prompt engineering for large language models-based code review automation},
  author={Pornprasit, Chanathip and Tantithamthavorn, Chakkrit},
  journal={Information and Software Technology},
  volume={175},
  pages={107523},
  year={2024},
  publisher={Elsevier}
}

@article{wang2025cosmos,
  title={COSMOS: Predictable and Cost-Effective Adaptation of LLMs},
  author={Wang, Jiayu and Albarghouthi, Aws and Sala, Frederic},
  journal={arXiv preprint arXiv:2505.01449},
  year={2025}
}
